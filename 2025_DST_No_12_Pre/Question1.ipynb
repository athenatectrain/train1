{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35e89b0",
   "metadata": {},
   "source": [
    "# 演習問題1\n",
    "## Word2Vecを用いて単語の分散表現を得る\n",
    "\n",
    "下記のコードでエラーが出る場合は，コマンドプロンプトで実行するか，`pip install gensim=4.3.3 --user`に書き換えて実行して下さい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim=4.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35ff14",
   "metadata": {},
   "source": [
    "### 使用データセット\n",
    "### **livedoor ニュースコーパス**\n",
    "NHN Japan株式会社が運営するlivedoor ニュースの記事から可能な限りHTMLタグを取り除いて作成したもの．<br>\n",
    "ニュース記事は分野ごとに分けられており，今回は\"ITライフハック\"についての記事をコーパスに用いる．<br>\n",
    "\n",
    "参考記事：<a>https://www.rondhuit.com/download.html#ldcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import ipadic\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "#rcParamsに文字化けしないようにフォントの設定を行う\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d234f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析器の初期化\n",
    "tagger = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "tagger.parse(\"\")  # MeCabの初期化（空文字列を解析することで初期化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ad236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズ関数\n",
    "def tokenize(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    words = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        if pos in [\"名詞\", \"接頭詞\", \"動詞\", \"形容詞\", \"副詞\", \"助詞\", \"助動詞\", \"連体詞\", \"接続詞\", \"感動詞\", \"記号\", \"フィラー\"]:\n",
    "            words.append(node.surface)\n",
    "        node = node.next\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を入力しトークナイズができるか確認\n",
    "print(tokenize(\"ここに文章を入力\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3816f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパスの読み込み\n",
    "df = pd.read_csv(\"./csv/it-life-hack.csv\")\n",
    "\n",
    "# トークン化\n",
    "sentences = df[\"body\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c0e81",
   "metadata": {},
   "source": [
    "### 元文章とトークン化した文章を見比べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c075494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元文章を表示\n",
    "print(\"元文章：\")\n",
    "print(df[\"body\"][0])\n",
    "\n",
    "print(\"\")\n",
    "# トークン化した結果を表示\n",
    "print(\"トークン化結果：\")\n",
    "before_sentence = sentences[0]\n",
    "print(before_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562abda8",
   "metadata": {},
   "source": [
    "### 問1 Word2Vecを用いてモデルを作成\n",
    "\n",
    "今回は，パラメータを vector_size=100, window=5, min_count=1, sg=0, seed=42に設定して学習を実行してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56985b54",
   "metadata": {},
   "source": [
    "### 問2 任意の単語が持つ単語ベクトルを取得\n",
    "今回はtarget_wordを\"スマホ\"とします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d25ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットワードを設定\n",
    "target_word = \"スマホ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = \n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23210c61",
   "metadata": {},
   "source": [
    "### 問3 任意の単語と類似度の高い単語上位10個を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a220bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の単語と類似度が高い単語トップ10を表示\n",
    "print(f\"\\\"{target_word}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in 解答欄:\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd21cf9",
   "metadata": {},
   "source": [
    "### 問4 任意の2単語間における類似度を表示\n",
    "今回はtarget_word2を\"難しい\"とします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットワード2を設定\n",
    "target_word2 = \"難しい\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の2単語の類似度を表示\n",
    "print(f\"\\\"{target_word}\\\"と\\\"{target_word2}\\\"の類似度\")\n",
    "print(解答欄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b22b7",
   "metadata": {},
   "source": [
    "### 問5 任意の2単語のベクトルの差を取り，結果ベクトルの周辺単語を表示\n",
    "今回は\"スマホ\"-\"難しい\"を計算してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98340e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの差を計算\n",
    "print(f\"\\\"{target_word}\\\"-\\\"{target_word2}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in 解答欄:\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbad142",
   "metadata": {},
   "source": [
    "### 問6 学習済みモデルを保存\n",
    "モデル名は\"simple_model.model\"にしてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f07539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(解答欄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6ba99",
   "metadata": {},
   "source": [
    "## 前処理の導入\n",
    "ipadeic辞書は品詞の大枠を[名詞, 接頭辞, 動詞, 形容詞, 副詞, 助詞, 助動詞, 連体詞, 接続詞, 感動詞, 記号, フィラー]で分類する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de44905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipadeicの辞書が分類する品詞を表示\n",
    "parsed = tagger.parse(\"人間 お 働く 忙しい まるで を らしい この そして はぁ ! あのー\")\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859d13f",
   "metadata": {},
   "source": [
    "### 問7 品詞, ストップワードの除去\n",
    "`stop_words.txt`を読み込みストップワード(不要単語)を除去する．<br>\n",
    "英単語を正規化し，数字や空白を削除する．<br>\n",
    "意味の重要度が高い可能性がある\"名詞\", \"動詞\", \"形容詞\"のみを使う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31276498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析器の初期化\n",
    "tagger = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "tagger.parse(\"\")  # MeCabの初期化（空文字列を解析することで初期化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードの読み込み\n",
    "# \"stop_words.txt\"をパス指定する\n",
    "with open(解答欄1, encoding=\"utf-8\") as f:\n",
    "    stop_words = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "# 英数字・記号などの正規化関数\n",
    "def clean_token(surface):\n",
    "    # 英字を小文字に正規化\n",
    "    surface = surface.lower()\n",
    "    # 数字は削除（全角・漢数字も含む）\n",
    "    surface = re.sub(r'[0-9０-９一二三四五六七八九十百千万億兆]+', '', surface)\n",
    "    # 記号・空白・改行を除去\n",
    "    surface = re.sub(r'[^\\wぁ-んァ-ン一-龥ー]+', '', surface)\n",
    "    return surface\n",
    "\n",
    "# トークナイズ＋クレンジング\n",
    "def tokenize_clean(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        surface = node.surface\n",
    "        features = node.feature.split(',')\n",
    "        # 品詞取得\n",
    "        pos = features[0]\n",
    "        # 原形取得\n",
    "        if len(features) > 6 and features[6] != '*':\n",
    "            base = features[6]\n",
    "        else:\n",
    "            base = surface\n",
    "        norm = clean_token(base)\n",
    "        # 指定する品詞（今回は\"名詞\", \"動詞\", \"形容詞\"）に含まれる\n",
    "        if pos in 解答欄2:\n",
    "            # norm が削除されず存在している\n",
    "            if norm:\n",
    "                # norm がstop_words に含まれていない\n",
    "                if norm not in stop_words:\n",
    "                    tokens.append(norm)\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e27bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズを含めた前処理の実行\n",
    "sentences = df[\"body\"].apply(tokenize_clean)\n",
    "# ストップワード除去前後の結果を表示\n",
    "print(\"ストップワード除去前：\")\n",
    "print(before_sentence)\n",
    "print(\"ストップワード除去後：\")\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46d94a",
   "metadata": {},
   "source": [
    "### 前処理したsentencesでWord2Vecを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の単語と類似度が高い単語トップ10を表示\n",
    "print(f\"\\\"{target_word}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in model.wv.most_similar(target_word, topn=10):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の2単語の類似度を表示\n",
    "print(f\"\\\"{target_word}\\\"と\\\"{target_word2}\\\"の類似度\")\n",
    "print(model.wv.similarity(target_word, target_word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348877aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの差を計算\n",
    "print(f\"\\\"{target_word}\\\"-\\\"{target_word2}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in model.wv.most_similar(positive=[target_word], negative=[target_word2]):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e9635",
   "metadata": {},
   "source": [
    "## ストップワード除去前と後で単語ベクトルの結果の違いを確認\n",
    "### 問8 学習済みモデルを読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードの有無による語彙数の確認\n",
    "print(f\"ストップワード除去前の語彙数：{len(simple_model.wv)}\")\n",
    "print(f\"ストップワード除去後の語彙数：{len(model.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の単語と類似度が高い単語トップ10を表示\n",
    "print(f\"\\\"{target_word}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "print(\"ストップワード除去前：\")\n",
    "for ranking in simple_model.wv.most_similar(target_word, topn=10):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")\n",
    "print(\"\")\n",
    "print(\"ストップワード除去後：\")\n",
    "for ranking in model.wv.most_similar(target_word, topn=10):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の2単語の類似度を表示\n",
    "print(f\"\\\"{target_word}\\\"と\\\"{target_word2}\\\"の類似度\")\n",
    "print(\"ストップワード除去前：\")\n",
    "print(simple_model.wv.similarity(target_word, target_word2))\n",
    "print(\"ストップワード除去後：\")\n",
    "print(model.wv.similarity(target_word, target_word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acdd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの差を計算\n",
    "print(f\"\\\"{target_word}\\\"-\\\"{target_word2}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "print(\"ストップワード除去前：\")\n",
    "for ranking in simple_model.wv.most_similar(positive=[target_word], negative=[target_word2]):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")\n",
    "print(\"\")\n",
    "print(\"ストップワード除去後：\")\n",
    "for ranking in model.wv.most_similar(positive=[target_word], negative=[target_word2]):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6e2ae",
   "metadata": {},
   "source": [
    "## 考察問題\n",
    "### 問9 内省的評価によって前処理前と後のモデルの精度を評価してください"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d4a35",
   "metadata": {},
   "source": [
    "回答欄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8545e8b",
   "metadata": {},
   "source": [
    "### 問10 Word2Vecを用いた自然言語処理において前処理が重要な理由を考察してください"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257be93c",
   "metadata": {},
   "source": [
    "回答欄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc78aa",
   "metadata": {},
   "source": [
    "## 時間が余った場合\n",
    "自分でtarget_wordを設定してみてください．<br>\n",
    "学習に使われなかった単語をキーにすると以下のようなエラーが出ます．<br>\n",
    "`KeyError: \"Key '???' not present in vocabulary\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd953557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の単語が語彙に存在するか確認(Trueなら存在， Falseなら不在)\n",
    "\"単語\" in model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58800da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word1 = \"pc\"\n",
    "target_word2 = \"it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の単語と類似度が高い単語トップ10を表示\n",
    "print(f\"\\\"{target_word1}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in model.wv.most_similar(target_word1, topn=10):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10718b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意の2単語の類似度を表示\n",
    "print(f\"\\\"{target_word1}\\\"と\\\"{target_word2}\\\"の類似度\")\n",
    "print(model.wv.similarity(target_word, target_word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの差を計算\n",
    "print(f\"\\\"{target_word1}\\\"-\\\"{target_word2}\\\"と類似度の高い単語トップ10\")\n",
    "print(\"単語\\t :類似度(最大値1, 最小値-1)\")\n",
    "for ranking in model.wv.most_similar(positive=[target_word1], negative=[target_word2]):\n",
    "    print(f\"{ranking[0]}\\t :{ranking[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
